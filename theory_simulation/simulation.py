import torch
import torch.nn as nn
import torch.optim as optim
from losses import get_loss_fn
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import argparse

params = {
    'lines.linewidth': 1.5,
    'legend.fontsize': 15,
    'axes.labelsize': 18,
    'axes.titlesize': 18,
    'xtick.labelsize': 20,
    'ytick.labelsize': 20,
}
matplotlib.rcParams.update(params)

plt.rc('font', family='Times New Roman')

parser = argparse.ArgumentParser()
parser.add_argument("--seed", type=int, default=42)
parser.add_argument("--noise_scale", type=float, default=2.0, help="Scale of the heavy-tailed noise")
parser.add_argument("--noise_prob", type=float, default=0.2, help="Probability of adding heavy-tailed noise")
parser.add_argument("--noise_threshold", type=float, default=1.5, help="X threshold for increased noise probability")
parser.add_argument("--n_samples", type=int, default=10, help="Number of training samples")
args = parser.parse_args()

torch.manual_seed(args.seed)
np.random.seed(args.seed)

def ground_truth(x):
    """Ground truth function"""
    return x ** 2 

def generate_heavy_tailed_noise(x_values, scale=1.0, base_prob=0.2, threshold=1.5, seed=None):
    """Generate heavy-tailed noise using Student's t-distribution with position-dependent sign"""
    if seed is not None:
        np.random.seed(seed)
    
    size = len(x_values)
    
    rng = np.random.RandomState()  
    noise_mask = rng.random(size) < base_prob
    
    noise = np.abs(np.random.standard_t(df=2, size=size)) * scale
    noise[~noise_mask] = 0  
    
    noise[x_values > threshold] *= -1  
    return noise

def generate_dataset(n_samples, noise_scale, noise_prob, noise_threshold, seed=None):
    
    if seed is not None:
        np.random.seed(seed)
    
    x = np.random.uniform(0, 3, n_samples)
    
    y_true = ground_truth(x)
    
    
    noise = generate_heavy_tailed_noise(
        x,
        scale=noise_scale,
        base_prob=noise_prob,
        threshold=noise_threshold,
        seed=seed
    )
    y_noisy = y_true + noise
    
    return list(zip(x, y_noisy))

COLORS = {
    'true': '#2C3E50',  
    'data': '#E74C3C',  
    'mse': '#3498DB',   
    'rankcosine': '#2ECC71',  
}

class LinearModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)
    
    def forward(self, x):
        return self.linear(x)

def train_model(data_points, loss_fn, learning_rate=0.001, epochs=1000):
    x = torch.tensor([[x] for x, _ in data_points], dtype=torch.float32)
    y = torch.tensor([[y] for _, y in data_points], dtype=torch.float32)
    
    model = LinearModel()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    for epoch in range(epochs):
        y_pred = model(x)
        loss = loss_fn(y_pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
    
    w = model.linear.weight.item()
    b = model.linear.bias.item()
    
    return model, w, b

def mse_closed_form(data_points):
    X = np.array([[x] for x, _ in data_points])
    y = np.array([y for _, y in data_points])
    
    X_b = np.c_[np.ones(X.shape[0]), X]
    
    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
    
    b, w = theta[0], theta[1]
    
    model = LinearModel()
    with torch.no_grad():
        model.linear.weight.data = torch.tensor([[w]], dtype=torch.float32)
        model.linear.bias.data = torch.tensor([b], dtype=torch.float32)
    
    return model, w, b

def plot_models(data, models_dict, equations):
    plt.figure(figsize=(8, 6))
    plt.rc('font', family='Times New Roman')
    
    plt.gcf().patch.set_facecolor('white')
    plt.gca().set_facecolor('white')
    
    plt.grid(True, linestyle='--', color='black', alpha=0.2)
    
    x_true = np.linspace(0, 3, 100)
    y_true = ground_truth(x_true)
    plt.plot(x_true, y_true, '-', color=COLORS['true'], 
             label='Ground Truth: f(x) = xÂ²', linewidth=2.5, alpha=0.8)
    
    x_data, y_data = zip(*data)
    plt.scatter(x_data, y_data, c=COLORS['data'], s=100,
               label='Training Data', alpha=0.7, edgecolors='white')
    
    x_pred = np.linspace(0, 3, 100)
    x_pred_tensor = torch.tensor([[x] for x in x_pred], dtype=torch.float32)
    
    for name, model in models_dict.items():
        with torch.no_grad():
            y_pred = model(x_pred_tensor).numpy()
        plt.plot(x_pred, y_pred, '-', color=COLORS[name.lower()], 
                label=f'{name}: {equations[name]}', linewidth=2, alpha=0.8)
    
    plt.xlabel('$x$')
    plt.ylabel('$\hat{f}(x)$')
    title = (f'Comparison of Different Loss Functions\n'
             f'(noise scale={args.noise_scale}, base_prob={args.noise_prob}, '
             f'threshold={args.noise_threshold}, n={args.n_samples})')
    plt.title(title)
    
    plt.legend(frameon=True, fancybox=True, shadow=True)
    
    plt.xlim(0, 3.1)
    y_min = min(min(min([model(torch.tensor([[3.0]])).item() for model in models_dict.values()]), min(y_data)), 0) -1
    y_max = max(max(max([model(torch.tensor([[3.0]])).item() for model in models_dict.values()]), max(y_data)) ,9) + 1
    plt.ylim(y_min, y_max)
    
    ax = plt.gca()
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_color('black')
    ax.spines['bottom'].set_color('black')
    
    plt.tight_layout()
    plt.savefig(f"Simulation_noise{args.noise_scale}_prob{args.noise_prob}_thresh{args.noise_threshold}_n{args.n_samples}.png")
    print(f"Simulation_noise{args.noise_scale}_prob{args.noise_prob}_thresh{args.noise_threshold}_n{args.n_samples}.png")
    
def format_equation(w, b):
    if b >= 0:
        return f'y = {w:.2f}x + {b:.2f}'
    else:
        return f'y = {w:.2f}x - {abs(b):.2f}'

if __name__ == "__main__":
    data = generate_dataset(
        n_samples=args.n_samples,
        noise_scale=args.noise_scale,
        noise_prob=args.noise_prob,
        noise_threshold=args.noise_threshold,
        seed=args.seed
    )
    
    mse_loss = nn.MSELoss()
    rankcosine_loss = get_loss_fn("rankcosine")
    listnet_loss = get_loss_fn("listnet")
    
    print("Training with MSE loss:")
    # model_mse, w_mse, b_mse = train_model(data, mse_loss)
    # mse_eq = format_equation(w_mse, b_mse)
    # print(f"MSE model: {mse_eq}\n")
    model_mse_closed, w_mse_closed, b_mse_closed = mse_closed_form(data)
    mse_closed_eq = format_equation(w_mse_closed, b_mse_closed)
    print(f"MSE closed-form solution: {mse_closed_eq}\n")
    
    print("Training with RankCosine loss:")
    model_rankcosine, w_rc, b_rc = train_model(data, rankcosine_loss)
    rc_eq = format_equation(w_rc, b_rc)
    print(f"RankCosine model: {rc_eq}\n")
    
    models = {
        'MSE': model_mse_closed,
        'RankCosine': model_rankcosine,
    }
    
    equations = {
        'MSE': mse_closed_eq,
        'RankCosine': rc_eq,
    }
    
    plot_models(data, models, equations)